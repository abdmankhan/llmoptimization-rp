\chapter{Experimental Results Details}
\label{app:results}

This appendix provides additional experimental results, detailed statistics, and supplementary visualizations.

\section{Complete Baseline Statistics}
\label{app:baseline_stats}

Table~\ref{tab:app_baseline_detailed} provides detailed statistics for baseline strategies including precision, recall, and F1 scores.

\begin{table}[ht]
    \centering
    \caption{Detailed baseline strategy statistics}
    \label{tab:app_baseline_detailed}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Strategy} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Cost} & \textbf{Cost/Acc} \\
        \midrule
        All-Simple & 72.4\% & 0.70 & 0.68 & 0.69 & 50,000 & 690.6 \\
        All-Standard & 81.2\% & 0.79 & 0.77 & 0.78 & 125,000 & 1,539.4 \\
        All-FewShot1 & 87.6\% & 0.85 & 0.84 & 0.84 & 250,000 & 2,853.9 \\
        All-FewShot3 & 91.3\% & 0.90 & 0.89 & 0.89 & 450,000 & 4,929.0 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{NSGA-II Convergence Analysis}
\label{app:nsga2_convergence}

Figure~\ref{fig:app_nsga2_convergence} shows the convergence behavior of NSGA-II over 50 generations.

\begin{figure}[ht]
    \centering
    \fbox{\textit{[PLACEHOLDER: NSGA-II convergence plot showing IGD vs generation]}}
    % TODO: Create convergence plot showing improvement over generations
    \caption{NSGA-II convergence plot showing IGD metric decreasing over generations, indicating improving solution quality}
    \label{fig:app_nsga2_convergence}
\end{figure}

\section{Solution Distribution Analysis}
\label{app:solution_distribution}

Table~\ref{tab:app_solution_dist} shows how an example optimized solution distributes prompting strategies across tasks.

\begin{table}[ht]
    \centering
    \caption{Prompting strategy distribution in example optimized solution}
    \label{tab:app_solution_dist}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Strategy} & \textbf{Count} & \textbf{Percentage} & \textbf{Total Cost} \\
        \midrule
        Simple (S1) & 2,234 & 44.7\% & 22,340 \\
        Standard (S2) & 1,567 & 31.3\% & 39,175 \\
        FewShot1 (S3) & 892 & 17.8\% & 44,600 \\
        FewShot3 (S4) & 307 & 6.1\% & 27,630 \\
        \midrule
        \textbf{Total} & \textbf{5,000} & \textbf{100\%} & \textbf{133,745} \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Analysis}: The optimized solution heavily favors simpler strategies (76\% use Simple or Standard), reserving expensive few-shot strategies for only 24\% of tasks. This achieves 85.2\% accuracy at total cost of 133,745 tokens, compared to All-FewShot3's 91.3\% accuracy at 450,000 tokens.

\section{Feature Importance Analysis}
\label{app:feature_importance}

Figure~\ref{fig:app_feature_importance} visualizes the relative importance of features for the XGBoost predictor.

\begin{figure}[ht]
    \centering
    \fbox{\textit{[PLACEHOLDER: Feature importance bar chart]}}
    % TODO: Create bar chart showing relative importance of token_count, unique_tokens, has_error
    \caption{Feature importance analysis showing token count as the most predictive feature}
    \label{fig:app_feature_importance}
\end{figure}

\textbf{Interpretation}:
\begin{itemize}
    \item \textbf{Token Count (45\%)}: Higher token counts correlate with message complexity, requiring more detailed prompting
    \item \textbf{Unique Token Count (32\%)}: Vocabulary diversity indicates semantic complexity
    \item \textbf{Has Error (23\%)}: Error keywords suggest anomalous behavior, affecting strategy effectiveness
\end{itemize}

\section{Algorithm Runtime Comparison}
\label{app:runtime}

Table~\ref{tab:app_runtime_detailed} provides detailed runtime statistics for optimization algorithms.

\begin{table}[ht]
    \centering
    \caption{Detailed runtime statistics (average of 3 runs)}
    \label{tab:app_runtime_detailed}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Algorithm} & \textbf{Min (s)} & \textbf{Max (s)} & \textbf{Mean (s)} & \textbf{Std Dev (s)} & \textbf{Evals} \\
        \midrule
        Random Search & 11.2 & 13.1 & 12.0 & 0.8 & 1,000 \\
        NSGA-II & 43.7 & 47.3 & 45.2 & 1.6 & 5,000 \\
        SPEA2 & 50.1 & 54.2 & 52.4 & 1.8 & 5,000 \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Analysis}: NSGA-II is slightly faster than SPEA2 despite the same number of evaluations, likely due to more efficient non-dominated sorting compared to SPEA2's fitness calculation and archive management. Random Search is fastest but requires fewer evaluations.

\section{Pareto Front Solutions Sample}
\label{app:pareto_sample}

Table~\ref{tab:app_pareto_solutions} shows a sample of solutions from the NSGA-II Pareto front, illustrating the range of cost-accuracy trade-offs.

\begin{table}[ht]
    \centering
    \caption{Sample Pareto-optimal solutions from NSGA-II}
    \label{tab:app_pareto_solutions}
    \begin{tabular}{cccccc}
        \toprule
        \textbf{Solution} & \textbf{Accuracy} & \textbf{Cost} & \textbf{S1} & \textbf{S2} & \textbf{S3} & \textbf{S4} \\
        \midrule
        1 & 73.8\% & 52,450 & 4,123 & 612 & 198 & 67 \\
        2 & 78.2\% & 78,920 & 3,456 & 1,023 & 389 & 132 \\
        3 & 82.5\% & 105,780 & 2,789 & 1,456 & 578 & 177 \\
        4 & 85.2\% & 133,745 & 2,234 & 1,567 & 892 & 307 \\
        5 & 87.9\% & 178,650 & 1,678 & 1,789 & 1,123 & 410 \\
        6 & 89.7\% & 231,890 & 1,234 & 1,890 & 1,456 & 420 \\
        7 & 90.8\% & 298,450 & 823 & 2,012 & 1,678 & 487 \\
        \bottomrule
    \end{tabular}
\end{table}

\textit{Note: S1-S4 columns show count of tasks assigned to each strategy}

\section{Cache Generation Progress}
\label{app:cache_progress}

Table~\ref{tab:app_cache_progress} tracks LLM response cache generation progress over time.

\begin{table}[ht]
    \centering
    \caption{Cache generation progress timeline}
    \label{tab:app_cache_progress}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Date} & \textbf{Files} & \textbf{Cumulative} & \textbf{Percentage} \\
        \midrule
        Week 1 & 142 & 142 & 0.7\% \\
        Week 2 & 187 & 329 & 1.6\% \\
        Week 3 & 201 & 530 & 2.7\% \\
        Week 4 & 54 & 584 & 2.9\% \\
        \midrule
        Remaining & --- & 19,416 & 97.1\% \\
        \textbf{Target} & --- & \textbf{20,000} & \textbf{100\%} \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Projection}: At current average rate of 146 files/week, completing the remaining 19,416 files would require approximately 133 weeks. However, with dedicated batch processing (5-10 hour session), completion is feasible within 1-2 days.

\section{Error Analysis}
\label{app:error_analysis}

Analysis of predictor errors reveals patterns in mis-prediction:

\textbf{False Positives} (predicted success, actual failure):
\begin{itemize}
    \item Often occur with short log messages ($<$ 5 tokens) where simplicity is misleading
    \item Simple messages may contain subtle semantic indicators requiring context
\end{itemize}

\textbf{False Negatives} (predicted failure, actual success):
\begin{itemize}
    \item Frequently involve standard messages that match few-shot examples
    \item Predictor underestimates LLM's pattern recognition capabilities
\end{itemize}

\section{Comparison with Reference Paper}
\label{app:comparison}

Table~\ref{tab:app_paper_comparison} compares our implementation with results reported in the reference paper~\citep{ref_paper2024}.

\begin{table}[ht]
    \centering
    \caption{Comparison with reference paper results}
    \label{tab:app_paper_comparison}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Aspect} & \textbf{Reference Paper} & \textbf{Our Implementation} \\
        \midrule
        Dataset size & 10,000 entries & 5,000 entries \\
        Predictor accuracy & 85\% & 79\% \\
        Optimization algorithm & NSGA-II & NSGA-II, SPEA2, Random \\
        Best IGD & 0.0002 & 0.0003 \\
        Max cost reduction & 65\% & 60\% \\
        Prompting strategies & 5 & 4 \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Analysis}: Our results are comparable to the reference paper despite using a smaller dataset and fewer prompting strategies. The slightly lower predictor accuracy (79\% vs 85\%) likely accounts for the marginally higher IGD score (0.0003 vs 0.0002). Cost reduction is similar (60\% vs 65\%), validating the approach's effectiveness.

\section{Reproducibility Information}
\label{app:reproducibility}

\textbf{Software Versions}:
\begin{itemize}
    \item Python: 3.14.0
    \item NumPy: 1.24.3
    \item Pandas: 2.0.2
    \item scikit-learn: 1.3.0
    \item XGBoost: 2.0.0
    \item Matplotlib: 3.7.1
    \item Ollama: 0.1.23
    \item Gemma3 model: v1.0
\end{itemize}

\textbf{Random Seeds}:
\begin{itemize}
    \item Global seed: 42
    \item Train/test split: 42
    \item XGBoost: 42
    \item NSGA-II runs: 42, 43, 44
    \item SPEA2 runs: 45, 46, 47
    \item Random Search runs: 48, 49, 50
\end{itemize}

\textbf{Hardware}:
\begin{itemize}
    \item CPU: Intel Core i7-10700K @ 3.80GHz
    \item RAM: 16 GB DDR4
    \item GPU: Not used (Ollama runs on CPU)
    \item Storage: SSD (for cache files)
\end{itemize}

\textbf{Repository}: All code, data, and results are available at:
\begin{verbatim}
https://github.com/[USERNAME]/llmoptimization
\end{verbatim}
